# Import Sample Data
./run.sh resume
service mysql start / restart
mysql -h hadoop_airflow_setup_mysql_1 -u root < sample_db.sql



# Sqoop Import Commands

## Bulk copy whole table
sqoop import \
    --username root \
    --num-mappers 1 \
    --connect jdbc:mysql://hadoop_airflow_setup_mysql_1:3306/training \
    --target-dir hdfs:///training/customer/data \
    --as-textfile \
    --table customer \

## Bulk copy using a query
sqoop import \
    --username root \
    --num-mappers 1 \
    --connect jdbc:mysql://hadoop_airflow_setup_mysql_1:3306/training \
    --target-dir hdfs:///training/customer \
    --as-textfile \
    --query "SELECT * FROM training.customer WHERE $CONDITIONS" \



# Useful HDFS Commands
hdfs dfs -ls /
hdfs dfs -mkdir /my_data
hdfs dfs -ls /
hdfs dfs -copyFromLocal /root/notebooks/sample_file.txt /my_data
hdfs dfs -ls /my_data
hdfs dfs -rm -r /my_data



# Setup Hive Metadatabase
schematool -initSchema -dbType mysql



# Hive Commands
hive
hive-cli >: CREATE DATABASE training;
hive-cli >: use training;
hive-cli >: 
CREATE TABLE `training.customer` (
  CUST_CODE string,
  CUST_NAME string,
  CUST_CITY string,
  WORKING_AREA string,
  CUST_COUNTRY string,
  GRADE int,
  OPENING_AMT string,
  RECEIVE_AMT string,
  PAYMENT_AMT string,
  OUTSTANDING_AMT string,
  PHONE_NO string,
  AGENT_CODE string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\,'

hive-cli >: SHOW CREATE TABLE customer;
hive-cli >: LOAD DATA INPATH '/training/customers/data' OVERWRITE INTO TABLE customer;
hive-cli >: SELECT * FROM customer LIMIT 5;
hive-cli >: set hive.cli.print.header=true;



# Sqoop Export Commands
